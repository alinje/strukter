
/******************************************************************************
** DAT038, TDA417  Datastrukturer och algoritmer, LP2 2020
** Lab 3: Plagiarism detection
*******************************************************************************/

Name of lab group: [76]
Group member A:    [Aline Eikeland]
Group member B:    [Emil Lundquist]
Group member C:    [Louise Tranborg]


/******************************************************************************
** Task: Running the slow program
**
** 1. What is the complexity of findSimilarity?
**    Answer in terms of N, the total number of 5-grams in the input files.
**    Assume that the number of 5-grams that occur in more than one file
**    is a small constant - that is, there is not much plagiarised text.
******************************************************************************/

This is our result running the program on medium file:
Reading all input files took 0,21 seconds
Building n-gram index took 0,00 seconds  
Computing similarity scores took 465,22 seconds
Finding the most similar files took 0,03 seconds
In total the program took 465,47 seconds

Balance statistics:
  files: BST, size 110, height 109
  index: BST, size 0, height -1
  similarity: BST, size 874, height 873

Plagiarism report:
   80 similarity: documents\medium\Plagiarism.txt and documents\medium\Find me.txt
   72 similarity: documents\medium\Pippi Longstocking (novel).txt and documents\medium\Pippi Longstocking (1969 TV series).txt
   68 similarity: documents\medium\The Children of Noisy Village (film).txt and documents\medium\More About the Children of Noisy Village.txt        
   67 similarity: documents\medium\Convention on the Rights of the Child.txt and documents\medium\Children's rights.txt
   58 similarity: documents\medium\Pippi Longstocking (1969 film).txt and documents\medium\Pippi Goes on Board (film).txt
   52 similarity: documents\medium\Picture book.txt and documents\medium\Children's literature.txt

We found that findSimilarity has many nested loops which is a bad sign for the complexity. In Lab3.java we have
commented the complexity of each line. The first two loops takes D-time each ex. Our calculations ended up being:
D*D*K*K+Constant --> O(N^2)
We got the final complexity to: O(N^2)

/******************************************************************************
** 2. How long did the program take on the 'small' and 'medium' directories?
**    Is the ratio between the times what you would expect, given the complexity?
**    Explain very briefly why.
*******************************************************************************/
The small took 2.6 sec to run and the medium took 465 sec. 

small (N = 20,000), medium (N = 200,000), big (N = 2,000,000), and huge (N = 4,000,000)

Small:
N^2 = 20 000^2 = 400 000 000

This is how long a runtime of N took for the small document:
2,61/400,000,000 = 6.52  e-9 s

Medium:
N^2 = 200 000^2 = 40 000 000 000

40,000,000,000 * 6.52  e-9 s = 260.8 sec is our calculated time for how long the medium is expected to take
but we got 465 sec...

The medium file runs twice as slow as we calculated.
The time between the two sizes are a bit larger than expected, but we believe that can be because the amount of
plagiarised text differ. Our analysis works where the plagiarised text is constant, which it is not, therefore the
non-expected difference. Lots of other factors can also affect the result ofc.

/******************************************************************************
** 3. How long do you predict the program would take to run on
**    the 'huge' directory? Show your calculations.
*******************************************************************************/

This is how long a runtime of N took for the documents:
small - 2,61/400,000,000 =         6.52  e-9 s
medium - 456,57 / 40,000,000,000 =  1.141 e-8 s
We calculate from medium.

Huge:
N^2 = 4,000,000^2 = 16 000 000 000 000

16 000 000 000 000 * 1.141 e-8 s = 182560 sec, which is almost 51 hours :O

/******************************************************************************
** Task: Using binary search trees
**
** 4. Which of the trees usually become unbalanced?
******************************************************************************/
Documents/small
Balance statistics:
  files: BST, size 7, height 6

Documents/medium
Balance statistics:
  files: BST, size 110, height 109

Files has almost the same size and height! This means that the tree is heavily unbalanced!
The files (small, medium..) are ordered! So in the tree files, we will add each element in order! So the
tree is going to be really unbalanced!

/******************************************************************************
** 5 (optional). Is there a simple way to stop these trees becoming unbalanced?
******************************************************************************/
We can randomize the files beforehand, so the possibility to add them in order are less likely.

/******************************************************************************
** Task: Using scapegoat trees
**
** 6. Now what is the total complexity of buildIndex and findSimilarity?
**    Again, assume a total of N 5-grams, and a constant number of 5-grams
**    that occur in more than one file.
******************************************************************************/
BuildIndex: K*D*3logN --> O(N*logN)
Looping though all the keys takes K-time, Looping though the N-grams takes D-time.
Adding to a scapegoatTree takes logN. So the final result was K*D*3logN which can
be written as O(N*logN).

findSimilarity:
D*D*N*2*logD^2*logN --> O(D^2*N*logD*logN)
This is the worst case complexity of the method. D symbolizes the number of documents. But the array
"paths" will mostly contain few elements, it will mostly be small.
The most Ngrams will only have an array of length 1. These assumptions got we from the assumption
that it's a constant number of 5-grams that occur in more than one file.

Therefore can we write the complexity will mostly be O(Constant*N*logD*logN) -->
O(Constant*N*Constant*logN) --> O(Constant*N*logN) --> O(N*logN)

Our final calculated complexity for findSimilarity is therefore O(N*logN), because
we could simplify it from it worst case complexity due to the assumption we could take.

In Lab3.java we have commented the worst case complexity for each row in the method.

/******************************************************************************
** 7 (optional). What if the total similarity score is an arbitrary number S,
**               rather than a small constant?
******************************************************************************/

[...]



/******************************************************************************
** Appendix: General information
**
** A. Approximately how many hours did you spend on the assignment?
******************************************************************************/
[Aline Eikeland] [6]
[Emil Lundquist] [7]
[Louise Tranborg] [9]

/******************************************************************************
** B. Are there any known bugs / limitations?
******************************************************************************/
Not that we have noticed!


/******************************************************************************
** C. Did you collaborate with any other students on this lab?
**    If so, please write in what way you collaborated and with whom.
**    Also include any resources (including the web) that you may
**    may have used in creating your design.
******************************************************************************/
We asked for supervision for help.


/******************************************************************************
** D. Describe any serious problems you encountered.                    
******************************************************************************/
We found it difficult to calculate the complexity some times.


/******************************************************************************
** E. List any other comments here.
**    Feel free to provide any feedback on how much you learned 
**    from doing the assignment, and whether you enjoyed it.                                             
******************************************************************************/
[...]

